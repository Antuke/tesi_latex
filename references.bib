% FairFace Dataset
@article{fairface,
	author       = {Kimmo K{\"{a}}rkk{\"{a}}inen and
	Jungseock Joo},
	title        = {FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age},
	journal      = {CoRR},
	volume       = {abs/1908.04913},
	year         = {2019},
	url          = {http://arxiv.org/abs/1908.04913},
	eprinttype    = {arXiv},
	eprint       = {1908.04913},
	timestamp    = {Mon, 19 Aug 2019 13:21:03 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1908-04913.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% LAGENDA Dataset
@article{mivolo2023,
	Author = {Maksim Kuprashevich and Irina Tolstykh},
	Title = {MiVOLO: Multi-input Transformer for Age and Gender Estimation},
	Year = {2023},
	Eprint = {arXiv:2307.04616},
}

%ir50
@InProceedings{ir50,
	author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
	title = {ArcFace: Additive Angular Margin Loss for Deep Face Recognition},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2019}
}

% uw
@INPROCEEDINGS{uw,
	author={Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
	booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
	title={Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}, 
	year={2018},
	volume={},
	number={},
	pages={7482-7491},
	keywords={Task analysis;Uncertainty;Semantics;Geometry;Image segmentation;Computational modeling},
	doi={10.1109/CVPR.2018.00781}}

%ema
@article{ema,
	author = {Lakkapragada, Anish and Sleiman, Essam and Surabhi, Saimourya and Wall, Dennis},
	year = {2023},
	month = {06},
	pages = {16246-16247},
	title = {Mitigating Negative Transfer in Multi-Task Learning with Exponential Moving Average Loss Weighting Strategies (Student Abstract)},
	volume = {37},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	doi = {10.1609/aaai.v37i13.26983}
}

%mtlora
@inproceedings{mtlora,
	author = {Agiza, Ahmed and Neseem, Marina and Reda, Sherief},
	year = {2024},
	month = {06},
	pages = {16196-16205},
	title = {MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning},
	doi = {10.1109/CVPR52733.2024.01533}
}

% coral
@article{coral,
	title = {Rank consistent ordinal regression for neural networks with application to age estimation},
	journal = {Pattern Recognition Letters},
	volume = {140},
	pages = {325-331},
	year = {2020},
	issn = {0167-8655},
	doi = {https://doi.org/10.1016/j.patrec.2020.11.008},
	url = {https://www.sciencedirect.com/science/article/pii/S016786552030413X},
	author = {Wenzhi Cao and Vahid Mirjalili and Sebastian Raschka},
	keywords = {Deep learning, Ordinal regression, Convolutional neural networks, Age prediction, Machine learning, Biometrics},
}


% LAGENDA Dataset
@article{mivolo2024,
	Author = {Maksim Kuprashevich and Grigorii Alekseenko and Irina Tolstykh},
	Title = {Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation},
	Year = {2024},
	Eprint = {arXiv:2403.02302},
}

% VGGFace2 Dataset
@misc{vggface2,
	title={VGGFace2: A dataset for recognising faces across pose and age}, 
	author={Qiong Cao and Li Shen and Weidi Xie and Omkar M. Parkhi and Andrew Zisserman},
	year={2018},
	eprint={1710.08092},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1710.08092}, 
}


% pytorch_amp
@misc{pytorchamp,
	author       = {{PyTorch}},
	title        = {Automatic Mixed Precision package - torch.amp},
	url={https://arxiv.org/abs/1710.08092},
	note         = {Accessed: 2025-10-13}
}

% IMDB-clean Dataset
@article{imdb-clean,
	title={FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in the Wild}, 
	author={Yiming Lin and Jie Shen and Yujiang Wang and Maja Pantic},
	year={2021},
	eprint={2106.11145},
	journal={arXiv},
	primaryClass={cs.CV}
}


% UtkFace Dataset
@article{utkface,
	title={Age Progression/Regression by Conditional Adversarial Autoencoder},
	author={Zhifei Zhang and Yang Song and Hairong Qi},
	journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2017},
	pages={4352-4360},
	url={https://api.semanticscholar.org/CorpusID:810708}
}

% Raf-db Dataset
@article{raf-db,
	title={Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild},
	author={Shan Li and Weihong Deng and Junping Du},
	journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2017},
	pages={2584-2593},
	url={https://api.semanticscholar.org/CorpusID:11413183}
}


% Celeba
@inproceedings{celeba1,
	title={TediGAN: Text-Guided Diverse Face Image Generation and Manipulation},
	author={Xia, Weihao and Yang, Yujiu and Xue, Jing-Hao and Wu, Baoyuan},
	booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2021}
}

% Celeba
@article{celeba2,
	title={Towards Open-World Text-Guided Face Image Generation and Manipulation},
	author={Xia, Weihao and Yang, Yujiu and Xue, Jing-Hao and Wu, Baoyuan},
	journal={arxiv preprint arxiv: 2104.08910},
	year={2021}
}



% adamW
@misc{adamW,
	title={Decoupled Weight Decay Regularization}, 
	author={Ilya Loshchilov and Frank Hutter},
	year={2019},
	eprint={1711.05101},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1711.05101}, 
}

% LoRa
@misc{lora,
	title={LoRA: Low-Rank Adaptation of Large Language Models}, 
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2106.09685}, 
}


% MoE LoRa
@misc{moelora,
	title={MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models}, 
	author={Tongxu Luo and Jiahe Lei and Fangyu Lei and Weihao Liu and Shizhu He and Jun Zhao and Kang Liu},
	year={2024},
	eprint={2402.12851},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.12851}, 
}

% MTLoRa
@misc{agiza2024mtloralowrankadaptationapproach,
	title={MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning}, 
	author={Ahmed Agiza and Marina Neseem and Sherief Reda},
	year={2024},
	eprint={2403.20320},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2403.20320}, 
}

% LoRa for image-classification
@INPROCEEDINGS{lora_vit2,
	author={Tiwary, Anupam and Sarkar, Shek Diya and Singh, Aditya Pratap and Agarwal, Pankaj Kumar and Burman, Subham and Poddar, Rishab},
	booktitle={2025 8th International Conference on Electronics, Materials Engineering & Nano-Technology (IEMENTech)}, 
	title={Fine-Tuning Vision Transformer Using LoRA for Image Classification}, 
	year={2025},
	volume={},
	number={},
	pages={1-4},
	keywords={Training;Adaptation models;Computer vision;Accuracy;Translation;Computational modeling;Dogs;Transformers;Real-time systems;Image classification;Vision Transformers;Food101;Cats vs Dogs LoRA;hugging face;PEFT},
	doi={10.1109/IEMENTech65115.2025.10959407}}

% LoRa for image-classification
@Article{lora_vit1,
	AUTHOR = {Hong, Zhenchen and Xiong, Jingwei and Yang, Han and Mo, Yu K.},
	TITLE = {Lightweight Low-Rank Adaptation Vision Transformer Framework for Cervical Cancer Detection and Cervix Type Classification},
	JOURNAL = {Bioengineering},
	VOLUME = {11},
	YEAR = {2024},
	NUMBER = {5},
	ARTICLE-NUMBER = {468},
	URL = {https://www.mdpi.com/2306-5354/11/5/468},
	PubMedID = {38790335},
	ISSN = {2306-5354},
	DOI = {10.3390/bioengineering11050468}
}

%Dora
@misc{Dora,
	title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
	author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
	year={2024},
	eprint={2402.09353},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2402.09353}, 
}


% PEFT review
@misc{peft_review,
	title={Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment}, 
	author={Lingling Xu and Haoran Xie and Si-Zhao Joe Qin and Xiaohui Tao and Fu Lee Wang},
	year={2023},
	eprint={2312.12148},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2312.12148}, 
}


% LoRa teoria
@misc{lora_theory,
	title={LoRA Training Provably Converges to a Low-Rank Global Minimum or It Fails Loudly (But it Probably Won't Fail)}, 
	author={Junsu Kim and Jaeyeon Kim and Ernest K. Ryu},
	year={2025},
	eprint={2502.09376},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2502.09376}, 
}

% LoRA+
@misc{lora_plus,
	title={LoRA+: Efficient Low Rank Adaptation of Large Models}, 
	author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
	year={2024},
	eprint={2402.12354},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2402.12354}, 
}


@misc{lora_img,
	title={What is LoRA (low-rank adaption)?},
	author={Joshua Noble},
	url={https://www.ibm.com/think/topics/lora}
}


% swin
@misc{liu2021swintransformerhierarchicalvision,
	title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
	author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
	year={2021},
	eprint={2103.14030},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2103.14030}, 
}

@article{lora_in_vits, title={Parameter-Efficient Model Adaptation for Vision Transformers}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/25160}, DOI={10.1609/aaai.v37i1.25160}, abstractNote={In computer vision, it has achieved great transfer learning performance via adapting large-scale pretrained vision models (e.g., vision transformers) to downstream tasks. Common approaches for model adaptation either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient model adaptation strategies for vision transformers on the image classification task. We formulate efficient model adaptation as a subspace training problem and perform a comprehensive benchmarking over different efficient adaptation methods. We conduct an empirical study on each efficient model adaptation method focusing on its performance alongside parameter cost. Furthermore, we propose a parameter-efficient model adaptation framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation method. We analyze and compare our method with a diverse set of baseline model adaptation methods (including state-of-the-art methods for pretrained language models). Our method performs the best in terms of the tradeoff between accuracy and parameter efficiency across 20 datasets under the few-shot setting and 7 image classification datasets under the full-shot setting.}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={He, Xuehai and Li, Chunyuan and Zhang, Pengchuan and Yang, Jianwei and Wang, Xin Eric}, year={2023}, month={Jun.}, pages={817-825} }

@misc{lora_learnforgets,
	title={LoRA Learns Less and Forgets Less}, 
	author={Dan Biderman and Jacob Portes and Jose Javier Gonzalez Ortiz and Mansheej Paul and Philip Greengard and Connor Jennings and Daniel King and Sam Havens and Vitaliy Chiley and Jonathan Frankle and Cody Blakeney and John P. Cunningham},
	year={2024},
	eprint={2405.09673},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2405.09673}, 
}

@misc{shuttleworth2025loravsfinetuningillusion,
	title={LoRA vs Full Fine-tuning: An Illusion of Equivalence}, 
	author={Reece Shuttleworth and Jacob Andreas and Antonio Torralba and Pratyusha Sharma},
	year={2025},
	eprint={2410.21228},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2410.21228}, 
}



@article{survey_vit,
	title={A survey of the vision transformers and their CNN-transformer based variants},
	volume={56},
	ISSN={1573-7462},
	url={http://dx.doi.org/10.1007/s10462-023-10595-0},
	DOI={10.1007/s10462-023-10595-0},
	number={S3},
	journal={Artificial Intelligence Review},
	publisher={Springer Science and Business Media LLC},
	author={Khan, Asifullah and Rauf, Zunaira and Sohail, Anabia and Khan, Abdul Rehman and Asif, Hifsa and Asif, Aqsa and Farooq, Umair},
	year={2023},
	month=oct, pages={2917–2970} }

%dino-v3
@misc{dinov3,
	title={DINOv3}, 
	author={Oriane Siméoni and Huy V. Vo and Maximilian Seitzer and Federico Baldassarre and Maxime Oquab and Cijo Jose and Vasil Khalidov and Marc Szafraniec and Seungeun Yi and Michaël Ramamonjisoa and Francisco Massa and Daniel Haziza and Luca Wehrstedt and Jianyuan Wang and Timothée Darcet and Théo Moutakanni and Leonel Sentana and Claire Roberts and Andrea Vedaldi and Jamie Tolan and John Brandt and Camille Couprie and Julien Mairal and Hervé Jégou and Patrick Labatut and Piotr Bojanowski},
	year={2025},
	eprint={2508.10104},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2508.10104}, 
}

%poster++
@article{poster,
	title = {POSTER++: A simpler and stronger facial expression recognition network},
	journal = {Pattern Recognition},
	volume = {157},
	pages = {110951},
	year = {2025},
	issn = {0031-3203},
	doi = {https://doi.org/10.1016/j.patcog.2024.110951},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320324007027},
	author = {Jiawei Mao and Rui Xu and Xuesong Yin and Yuanqi Chang and Binling Nie and Aibin Huang and Yigang Wang},
	keywords = {Facial expression recognition, Industry deployment, Window attention mechanism},
}

%apvit
@ARTICLE{apvit,
	author={Xue, Fanglei and Wang, Qiangchang and Tan, Zichang and Ma, Zhongsong and Guo, Guodong},
	journal={IEEE Transactions on Affective Computing}, 
	title={Vision Transformer With Attentive Pooling for Robust Facial Expression Recognition}, 
	year={2023},
	volume={14},
	number={4},
	pages={3244-3256},
	keywords={Transformers;Feature extraction;Noise measurement;Task analysis;Face recognition;Computational modeling;Deep learning;Affect;attentive pooling;deep learning;facial expression recognition;vision transformer},
	doi={10.1109/TAFFC.2022.3226473}}

% resemotenet
@ARTICLE{resemotenet,
	author={Roy, Arnab Kumar and Kathania, Hemant Kumar and Sharma, Adhitiya and Dey, Abhishek and Ansari, Md. Sarfaraj Alam},
	journal={IEEE Signal Processing Letters}, 
	title={ResEmoteNet: Bridging Accuracy and Loss Reduction in Facial Emotion Recognition}, 
	year={2025},
	volume={32},
	number={},
	pages={491-495},
	keywords={Iron;Emotion recognition;Feature extraction;Convolutional neural networks;Accuracy;Training;Computer architecture;Residual neural networks;Facial features;Face recognition;Facial emotion recognition;convolutional neural network;squeeze and excitation network;residual network},
	doi={10.1109/LSP.2024.3521321}
}



% dinov2 meetstext
@misc{dinov2meetstext,
	title={DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment}, 
	author={Cijo Jose and Théo Moutakanni and Dahyun Kang and Federico Baldassarre and Timothée Darcet and Hu Xu and Daniel Li and Marc Szafraniec and Michaël Ramamonjisoa and Maxime Oquab and Oriane Siméoni and Huy V. Vo and Patrick Labatut and Piotr Bojanowski},
	year={2024},
	eprint={2412.16334},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2412.16334}, 
}

% llava
@misc{llava1,
	title={Visual Instruction Tuning}, 
	author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
	year={2023},
	eprint={2304.08485},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2304.08485}, 
}

@inproceedings{llava,
	author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
	pages = {34892--34916},
	publisher = {Curran Associates, Inc.},
	title = {Visual Instruction Tuning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
	volume = {36},
	year = {2023}
}


% blip2
@misc{blip2,
	title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
	author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
	year={2023},
	eprint={2301.12597},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2301.12597}, 
}

%pe
@misc{pe,
	title={Perception Encoder: The best visual embeddings are not at the output of the network}, 
	author={Daniel Bolya and Po-Yao Huang and Peize Sun and Jang Hyun Cho and Andrea Madotto and Chen Wei and Tengyu Ma and Jiale Zhi and Jathushan Rajasegaran and Hanoona Rasheed and Junke Wang and Marco Monteiro and Hu Xu and Shiyu Dong and Nikhila Ravi and Daniel Li and Piotr Dollár and Christoph Feichtenhofer},
	year={2025},
	eprint={2504.13181},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2504.13181}, 
}


%siglip2
@misc{siglip2,
	title={SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features}, 
	author={Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier Hénaff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai},
	year={2025},
	eprint={2502.14786},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2502.14786}, 
}

%clip
@misc{clip,
	title={Learning Transferable Visual Models From Natural Language Supervision}, 
	author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
	year={2021},
	eprint={2103.00020},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2103.00020}, 
}

%rope
@misc{rope,
	title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
	author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
	year={2023},
	eprint={2104.09864},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2104.09864}, 
}

% transformers
@misc{transformers1,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2023},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1706.03762}, 
}

@inproceedings{transformers,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Attention is All you Need},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	volume = {30},
	year = {2017}
}


%ViT
@misc{vit,
	title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
	author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	year={2021},
	eprint={2010.11929},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2010.11929}, 
}

%Slora
@article{slora,
	title={S-LoRA: Serving Thousands of Concurrent LoRA Adapters},
	author={Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman and Lee, Nicholas and Yang, Shuo and Chou, Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and Gonzalez, Joseph E. and Stoica, Ion},
	journal={arXiv preprint arXiv:2311.03285},
	year={2023}
}

%mlora
@misc{mlora,
	title={mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs}, 
	author={Zhengmao Ye and Dengchun Li and Zetao Hu and Tingfeng Lan and Jian Sha and Sicong Zhang and Lei Duan and Jie Zuo and Hui Lu and Yuanchun Zhou and Mingjie Tang},
	year={2024},
	eprint={2312.02515},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2312.02515}, 
}

%DINO
@misc{dinov1,
	title={Emerging Properties in Self-Supervised Vision Transformers}, 
	author={Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
	year={2021},
	eprint={2104.14294},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2104.14294}, 
}

%DINOV2
@misc{dinov2,
	title={DINOv2: Learning Robust Visual Features without Supervision}, 
	author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
	year={2024},
	eprint={2304.07193},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2304.07193}, 
}

%deit
@misc{deit,
	title={Training data-efficient image transformers and distillation through attention}, 
	author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Hervé Jégou},
	year={2021},
	eprint={2012.12877},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2012.12877}, 
}

@article{survey_vit_hvt,
	title={A survey of the vision transformers and their CNN-transformer based variants},
	volume={56},
	ISSN={1573-7462},
	url={http://dx.doi.org/10.1007/s10462-023-10595-0},
	DOI={10.1007/s10462-023-10595-0},
	number={S3},
	journal={Artificial Intelligence Review},
	publisher={Springer Science and Business Media LLC},
	author={Khan, Asifullah and Rauf, Zunaira and Sohail, Anabia and Khan, Abdul Rehman and Asif, Hifsa and Asif, Aqsa and Farooq, Umair},
	year={2023},
	month=oct, pages={2917–2970} }



%survey on self-supervised vit
@misc{sslvit,
	title={A Survey of the Self Supervised Learning Mechanisms for Vision Transformers}, 
	author={Asifullah Khan and Anabia Sohail and Mustansar Fiaz and Mehdi Hassan and Tariq Habib Afridi and Sibghat Ullah Marwat and Farzeen Munir and Safdar Ali and Hannan Naseem and Muhammad Zaigham Zaheer and Kamran Ali and Tangina Sultana and Ziaurrehman Tanoli and Naeem Akhter},
	year={2025},
	eprint={2408.17059},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2408.17059}, 
}

%DINOV3
@misc{dinov3,
	title={DINOv3}, 
	author={Oriane Siméoni and Huy V. Vo and Maximilian Seitzer and Federico Baldassarre and Maxime Oquab and Cijo Jose and Vasil Khalidov and Marc Szafraniec and Seungeun Yi and Michaël Ramamonjisoa and Francisco Massa and Daniel Haziza and Luca Wehrstedt and Jianyuan Wang and Timothée Darcet and Théo Moutakanni and Leonel Sentana and Claire Roberts and Andrea Vedaldi and Jamie Tolan and John Brandt and Camille Couprie and Julien Mairal and Hervé Jégou and Patrick Labatut and Piotr Bojanowski},
	year={2025},
	eprint={2508.10104},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2508.10104}, 
}

% Bert
@misc{beit,
	title={BEiT: BERT Pre-Training of Image Transformers}, 
	author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
	year={2022},
	eprint={2106.08254},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2106.08254}, 
}

@misc{swin,
	title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
	author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
	year={2021},
	eprint={2103.14030},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2103.14030}, 
}


@misc{BLoRA,
	author       = {Ali Sabet},
	title        = {{BLoRA}: Maximize GPU util by routing inference through multiple LoRAs in same batch.},
	year={2024},
	howpublished = {GitHub repository},
	url          = {https://github.com/sabetAI/BLoRA},
}

%qlora
@misc{qlora,
	title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
	author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
	year={2023},
	eprint={2305.14314},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2305.14314}, 
}

%Lorahub
@misc{huang2024lorahubefficientcrosstaskgeneralization,
	title={LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition}, 
	author={Chengsong Huang and Qian Liu and Bill Yuchen Lin and Tianyu Pang and Chao Du and Min Lin},
	year={2024},
	eprint={2307.13269},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2307.13269}, 
}

% robe su lora
@article{schulman2025lora,
	author = {John Schulman and Thinking Machines Lab},
	title = {LoRA Without Regret},
	journal = {Thinking Machines Lab: Connectionism},
	year = {2025},
	note = {https://thinkingmachines.ai/blog/lora/},
	doi = {10.64434/tml.20250929},
}



% vgg age 
@article{Greco2022,
	author    = {Greco, Antonio and Saggese, Alessia and Vento, Mario and Mazzeo, Pierluigi Luigi},
	title     = {Effective training of convolutional neural networks for age estimation based on knowledge distillation},
	journal   = {Neural Computing and Applications},
	volume    = {34},
	pages     = {21449--21464},
	year      = {2022},
	doi       = {10.1007/s00521-021-05981-0},
	url       = {https://doi.org/10.1007/s00521-021-05981-0}
}